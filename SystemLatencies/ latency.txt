L1 cache reference                            0.5 ns
Branch mispredict                             5   ns
L2 cache reference                            7   ns             14x L1 cache
Mutex lock/unlock                            25   ns
Main memory reference                       100   ns             20x L2 cache, 200x L1 cache
Compress 1K bytes with Zippy              3,000   ns
Send 1K bytes over 1 Gbps network        10,000   ns    0.01 ms
SSD random read                         150,000   ns
Read 1 MB sequentially from memory      250,000   ns    0.25 ms
Round trip within same datacenter       500,000   ns    0.5  ms
Read 1 MB sequentially from SSD       1,000,000   ns    1    ms  4X memory
Disk seek                            10,000,000   ns   10    ms  20x datacenter roundtrip
Read 1 MB sequentially from disk     20,000,000   ns   20    ms  80x memory, 20X SSD
Send packet CA->Netherlands->CA     150,000,000   ns  150    ms

1 ns = 10-9 seconds
1 ms = 10-3 seconds
Assuming ~1GB/sec SSD

By Jeff Dean (http://research.google.com/people/jeff/)
Originally by Peter Norvig (http://norvig.com/21-days.html#answers)
Some updates from: https://gist.github.com/2843375
Great 'humanized' comparison version: https://gist.github.com/2843375

Cache Levels
https://www.youtube.com/watch?v=qZTt8JtQOmw
The compiler knows at any time what is stored in which register and 
if it is not there, it gets it from the memory, which is cached on 
different levels.


(mostly from handmade hero)
There are two main types of speed, latency and throughput.
Latency (response time) is the time between the start of a process and its completion, 
say millisecs for a disk access. Time to start sth and finish it (1 thing).
It is quite high though, due to the physical distance (wires), that's why we 
have this cascading series of memory (caches) with L1 being into the chip
(physical distance is smaller). 
Throughput (bandwidth) is the total amount of work done per unit time (say MBytes/sec
for a disk transfer). How fast things can go at peak congestion: GPUs are all about 
throughput over latency; don’t wait for results that aren’t there yet, do something else instead!


xGHz -> clock cycle -> tics for doing operations
1 single tick of the CPU is the fastest CPU can do things
(perhaps 2 things in one cycle)
usually, at least one clock passes from the time the CPU starts and finished sth
(although, it can issue multiple stuff per clock)

so, intuitively the CPU can do one thing x billion times a sec (xGHz)
speed of light/xGHz -> how fast light can travel in the amount of time 
it wpuld take the CPU to execute one thing (~9cms for 3.2GHz)

Stacked DRAM (actual DRAM that sits on the CPU) so we don't have to go through the bus/wires
